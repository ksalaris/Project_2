{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a Storm database to store storm data \n",
    "\n",
    "import pymongo\n",
    "\n",
    "conn = 'mongodb://localhost:27017'\n",
    "client = pymongo.MongoClient(conn)\n",
    "\n",
    "db = client.StormDB\n",
    "\n",
    "data = db.data.find()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing and cleaning the historical file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BEGIN_YEARMONTH</th>\n",
       "      <th>BEGIN_DAY</th>\n",
       "      <th>BEGIN_TIME</th>\n",
       "      <th>END_YEARMONTH</th>\n",
       "      <th>END_DAY</th>\n",
       "      <th>END_TIME</th>\n",
       "      <th>EPISODE_ID</th>\n",
       "      <th>EVENT_ID</th>\n",
       "      <th>STATE</th>\n",
       "      <th>STATE_FIPS</th>\n",
       "      <th>...</th>\n",
       "      <th>END_RANGE</th>\n",
       "      <th>END_AZIMUTH</th>\n",
       "      <th>END_LOCATION</th>\n",
       "      <th>BEGIN_LAT</th>\n",
       "      <th>BEGIN_LON</th>\n",
       "      <th>END_LAT</th>\n",
       "      <th>END_LON</th>\n",
       "      <th>EPISODE_NARRATIVE</th>\n",
       "      <th>EVENT_NARRATIVE</th>\n",
       "      <th>DATA_SOURCE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>199901</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>199901</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>1501526</td>\n",
       "      <td>5683694</td>\n",
       "      <td>MASSACHUSETTS</td>\n",
       "      <td>25.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A strong high pressure system centered over so...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PDC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>199901</td>\n",
       "      <td>2</td>\n",
       "      <td>700</td>\n",
       "      <td>199901</td>\n",
       "      <td>2</td>\n",
       "      <td>2359</td>\n",
       "      <td>1500622</td>\n",
       "      <td>5683477</td>\n",
       "      <td>MICHIGAN</td>\n",
       "      <td>26.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Blizzard conditions developed across lower Mic...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PDC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>199901</td>\n",
       "      <td>13</td>\n",
       "      <td>130</td>\n",
       "      <td>199901</td>\n",
       "      <td>13</td>\n",
       "      <td>1400</td>\n",
       "      <td>1500140</td>\n",
       "      <td>5683047</td>\n",
       "      <td>OHIO</td>\n",
       "      <td>39.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Freezing rain and sleet changed to snow as a s...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PDC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>199908</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>199908</td>\n",
       "      <td>31</td>\n",
       "      <td>2359</td>\n",
       "      <td>2409403</td>\n",
       "      <td>5712941</td>\n",
       "      <td>MISSOURI</td>\n",
       "      <td>29.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>After an abnormally wet June, the rest of the ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PDC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>199901</td>\n",
       "      <td>2</td>\n",
       "      <td>2300</td>\n",
       "      <td>199901</td>\n",
       "      <td>3</td>\n",
       "      <td>1900</td>\n",
       "      <td>1502756</td>\n",
       "      <td>5681541</td>\n",
       "      <td>NEW YORK</td>\n",
       "      <td>36.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A deep area of low pressure moved from the Mis...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PDC</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 51 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   BEGIN_YEARMONTH  BEGIN_DAY  BEGIN_TIME  END_YEARMONTH  END_DAY  END_TIME  \\\n",
       "0           199901         14           0         199901       15         0   \n",
       "1           199901          2         700         199901        2      2359   \n",
       "2           199901         13         130         199901       13      1400   \n",
       "3           199908          1           1         199908       31      2359   \n",
       "4           199901          2        2300         199901        3      1900   \n",
       "\n",
       "   EPISODE_ID  EVENT_ID          STATE  STATE_FIPS     ...      END_RANGE  \\\n",
       "0     1501526   5683694  MASSACHUSETTS        25.0     ...            NaN   \n",
       "1     1500622   5683477       MICHIGAN        26.0     ...            NaN   \n",
       "2     1500140   5683047           OHIO        39.0     ...            NaN   \n",
       "3     2409403   5712941       MISSOURI        29.0     ...            NaN   \n",
       "4     1502756   5681541       NEW YORK        36.0     ...            NaN   \n",
       "\n",
       "  END_AZIMUTH END_LOCATION BEGIN_LAT  BEGIN_LON END_LAT END_LON  \\\n",
       "0         NaN          NaN       NaN        NaN     NaN     NaN   \n",
       "1         NaN          NaN       NaN        NaN     NaN     NaN   \n",
       "2         NaN          NaN       NaN        NaN     NaN     NaN   \n",
       "3         NaN          NaN       NaN        NaN     NaN     NaN   \n",
       "4         NaN          NaN       NaN        NaN     NaN     NaN   \n",
       "\n",
       "                                   EPISODE_NARRATIVE EVENT_NARRATIVE  \\\n",
       "0  A strong high pressure system centered over so...             NaN   \n",
       "1  Blizzard conditions developed across lower Mic...             NaN   \n",
       "2  Freezing rain and sleet changed to snow as a s...             NaN   \n",
       "3  After an abnormally wet June, the rest of the ...             NaN   \n",
       "4  A deep area of low pressure moved from the Mis...             NaN   \n",
       "\n",
       "  DATA_SOURCE  \n",
       "0         PDC  \n",
       "1         PDC  \n",
       "2         PDC  \n",
       "3         PDC  \n",
       "4         PDC  \n",
       "\n",
       "[5 rows x 51 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import glob loop through 20 years of storm data\n",
    "import glob\n",
    "\n",
    "path = \"Resources\" \n",
    "all_files = glob.glob(path + \"/*.csv\")\n",
    "li=[]\n",
    "\n",
    "for filename in all_files:\n",
    "    df = pd.read_csv(filename, index_col=None, header=0)\n",
    "    li.append(df)\n",
    "\n",
    "# Create a dataframe using the imported data\n",
    "storm_data = pd.concat(li, axis=0, ignore_index=True)\n",
    "\n",
    "storm_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parse year and month description\n",
    "storm_data['BEGIN_YEARMONTH']=storm_data['BEGIN_YEARMONTH'].astype(str)\n",
    "storm_data['BEGIN_YEAR']=storm_data['BEGIN_YEARMONTH'].str[0:4]\n",
    "storm_data['BEGIN_MONTH']=storm_data['BEGIN_YEARMONTH'].str[4:6]\n",
    "\n",
    "#Move the new columns to the first and second column\n",
    "cols = list(storm_data.columns)\n",
    "cols = [cols[-1]] + cols[:-1]\n",
    "storm_data = storm_data[cols]\n",
    "\n",
    "cols = list(storm_data.columns)\n",
    "cols = [cols[-1]] + cols[:-1]\n",
    "storm_data = storm_data[cols]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storm_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop columns that are not needed\n",
    "storm_data.drop(columns={'BEGIN_YEARMONTH','END_YEARMONTH','END_DAY','END_TIME','CZ_TYPE','CZ_FIPS','CZ_NAME','CZ_TIMEZONE','MAGNITUDE','MAGNITUDE_TYPE',\n",
    "                    'CATEGORY','TOR_F_SCALE','TOR_LENGTH','TOR_WIDTH','TOR_OTHER_WFO','TOR_OTHER_CZ_STATE',\n",
    "                    'TOR_OTHER_CZ_FIPS','TOR_OTHER_CZ_NAME','BEGIN_AZIMUTH','BEGIN_RANGE',\n",
    "                    'END_RANGE','END_AZIMUTH','END_LOCATION','EPISODE_NARRATIVE','EVENT_NARRATIVE',\n",
    "                    'DATA_SOURCE','CZ_TIMEZONE','BEGIN_DATE_TIME','END_DATE_TIME',\n",
    "                    'BEGIN_TIME','EPISODE_ID','STATE_FIPS','YEAR',\n",
    "                    'SOURCE','STATE','WFO'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storm_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retrive the total for damage and death\n",
    "storm_data[[\"DEATHS_DIRECT\",'DEATHS_INDIRECT','INJURIES_DIRECT','INJURIES_INDIRECT',\"DAMAGE_PROPERTY\",\"DAMAGE_CROPS\"]].fillna(0, inplace=True)\n",
    "storm_data[\"DEATH_TOLL\"]=storm_data['DEATHS_DIRECT']+storm_data['DEATHS_INDIRECT']\n",
    "storm_data['TOTAL_INJURIES']=storm_data['INJURIES_DIRECT']+storm_data['INJURIES_INDIRECT']\n",
    "\n",
    "#change property and crop damage to numbers and add 1000\n",
    "storm_data[\"DAMAGE_PROPERTY\"]=storm_data['DAMAGE_PROPERTY'].str[:-1]\n",
    "storm_data[\"DAMAGE_CROPS\"]=storm_data['DAMAGE_CROPS'].str[:-1]\n",
    "\n",
    "storm_data[\"DAMAGE_PROPERTY\"]=(storm_data['DAMAGE_PROPERTY'].apply(pd.to_numeric))\n",
    "storm_data[\"DAMAGE_CROPS\"]=(storm_data['DAMAGE_CROPS'].apply(pd.to_numeric))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storm_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Store storm data in mongo db\n",
    "storm_json= storm_data.to_dict(orient='records')\n",
    "db.storm_info.insert_many(storm_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storm_data.drop(columns={'INJURIES_DIRECT','INJURIES_INDIRECT','DEATHS_DIRECT','DEATHS_INDIRECT'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storm_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_groupby=storm_data[[\"EVENT_TYPE\",\"DAMAGE_PROPERTY\",\"DAMAGE_CROPS\",\"DEATH_TOLL\",'TOTAL_INJURIES']]\n",
    "sum_groupby[\"EVENT_TYPE\"]=sum_groupby[\"EVENT_TYPE\"].replace({\"Flash Flood\":\"Flood\",\"Excessive Heat\":'Heat' })\n",
    "sum_groupby=sum_groupby.groupby([\"EVENT_TYPE\"]).sum()\n",
    "\n",
    "count_groupby=storm_data[[\"EVENT_TYPE\",\"DAMAGE_PROPERTY\"]]\n",
    "count_groupby[\"EVENT_TYPE\"]=count_groupby[\"EVENT_TYPE\"].replace({\"Flash Flood\":\"Flood\"})\n",
    "count_groupby=count_groupby.groupby([\"EVENT_TYPE\"]).count()\n",
    "count_groupby.rename(columns={'DAMAGE_PROPERTY':'COUNT'}, inplace=True)\n",
    "\n",
    "storm_summary=pd.merge(count_groupby,sum_groupby, on=\"EVENT_TYPE\")\n",
    "\n",
    "storm_summary.sort_values(\"COUNT\", ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "storm_summary.sort_values(\"DAMAGE_PROPERTY\", ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storm_summary.sort_values(\"TOTAL_INJURIES\", ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Sumarize flood data\n",
    "flood_data= storm_data.loc[(storm_data['EVENT_TYPE'] == 'Flood') | (storm_data['EVENT_TYPE'] == 'Flash Flood')]\n",
    "\n",
    "flood_data.reset_index(inplace=True)\n",
    "flood_data.drop(columns={'index'},inplace=True)\n",
    "\n",
    "#sum_groupby=flood_data[[\"BEGIN_YEAR\",\"DAMAGE_PROPERTY\",\"DAMAGE_CROPS\",\"DEATH_TOLL\",'TOTAL_INJURIES']]\n",
    "#sum_groupby=sum_groupby.groupby([\"BEGIN_YEAR\"]).sum()\n",
    "#count_groupby=flood_data[[\"BEGIN_YEAR\",\"DAMAGE_PROPERTY\"]].groupby([\"BEGIN_YEAR\"]).count()\n",
    "#count_groupby.rename(columns={'DAMAGE_PROPERTY':'FLOOD_COUNT'}, inplace=True)\n",
    "\n",
    "#flood_summary=pd.merge(count_groupby,sum_groupby, on=\"BEGIN_YEAR\")\n",
    "#flood_summary.to_csv(\"Output/FloodSummary.csv\", index=True)\n",
    "\n",
    "flood_data= flood_data.loc[(flood_data['BEGIN_YEAR'] == '2019')]\n",
    "\n",
    "flood_data.to_csv(\"Output/FloodData2019.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sumarize Thunderstorm data\n",
    "thunderstorm_data= storm_data.loc[(storm_data['EVENT_TYPE'] == 'Thunderstorm Wind')]\n",
    "\n",
    "thunderstorm_data.reset_index(inplace=True)\n",
    "thunderstorm_data.drop(columns={'index'},inplace=True)\n",
    "\n",
    "sum_groupby=thunderstorm_data[[\"BEGIN_YEAR\",\"DAMAGE_PROPERTY\",\"DAMAGE_CROPS\",\"DEATH_TOLL\",'TOTAL_INJURIES']]\n",
    "sum_groupby=sum_groupby.groupby([\"BEGIN_YEAR\"]).sum()\n",
    "count_groupby=thunderstorm_data[[\"BEGIN_YEAR\",\"DAMAGE_PROPERTY\"]].groupby([\"BEGIN_YEAR\"]).count()\n",
    "count_groupby.rename(columns={'DAMAGE_PROPERTY':'FLOOD_COUNT'}, inplace=True)\n",
    "\n",
    "thunderstorm_summary=pd.merge(count_groupby,sum_groupby, on=\"BEGIN_YEAR\")\n",
    "thunderstorm_summary.to_csv(\"Output/ThunderstormSummary.csv\", index=True)\n",
    "\n",
    "thunderstorm_data= thunderstorm_data.loc[(thunderstorm_data['BEGIN_YEAR'] == '2019')]\n",
    "\n",
    "thunderstorm_data.to_csv(\"Output/ThunderstormData2019.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sumarize hail data\n",
    "hail_data= storm_data.loc[(storm_data['EVENT_TYPE'] == 'Hail')]\n",
    "\n",
    "hail_data.reset_index(inplace=True)\n",
    "hail_data.drop(columns={'index'},inplace=True)\n",
    "\n",
    "sum_groupby=hail_data[[\"BEGIN_YEAR\",\"DAMAGE_PROPERTY\",\"DAMAGE_CROPS\",\"DEATH_TOLL\",'TOTAL_INJURIES']].groupby([\"BEGIN_YEAR\"]).sum()\n",
    "count_groupby=hail_data[[\"BEGIN_YEAR\",\"DAMAGE_PROPERTY\"]].groupby([\"BEGIN_YEAR\"]).count()\n",
    "count_groupby.rename(columns={'DAMAGE_PROPERTY':'HAIL_COUNT'}, inplace=True)\n",
    "\n",
    "hail_summary=pd.merge(count_groupby,sum_groupby, on=\"BEGIN_YEAR\")\n",
    "hail_summary.to_csv(\"Output/HailSummary.csv\", index=True)\n",
    "\n",
    "hail_data= hail_data.loc[(hail_data['BEGIN_YEAR'] == '2019')]\n",
    "\n",
    "hail_data.to_csv(\"Output/HailData2019.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sumarize high wind data\n",
    "highwind_data= storm_data.loc[(storm_data['EVENT_TYPE'] == 'High Wind')]\n",
    "\n",
    "highwind_data.reset_index(inplace=True)\n",
    "highwind_data.drop(columns={'index'},inplace=True)\n",
    "\n",
    "sum_groupby=highwind_data[[\"BEGIN_YEAR\",\"DAMAGE_PROPERTY\",\"DAMAGE_CROPS\",\"DEATH_TOLL\",'TOTAL_INJURIES']].groupby([\"BEGIN_YEAR\"]).sum()\n",
    "count_groupby=highwind_data[[\"BEGIN_YEAR\",\"DAMAGE_PROPERTY\"]].groupby([\"BEGIN_YEAR\"]).count()\n",
    "count_groupby.rename(columns={'DAMAGE_PROPERTY':'HIGHWIND_COUNT'}, inplace=True)\n",
    "\n",
    "highwind_summary=pd.merge(count_groupby,sum_groupby, on=\"BEGIN_YEAR\")\n",
    "hail_summary.to_csv(\"Output/HighWindSummary.csv\", index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sumarize winter weather data\n",
    "winterweather_data= storm_data.loc[(storm_data['EVENT_TYPE'] == 'Winter Weather')]\n",
    "\n",
    "winterweather_data.reset_index(inplace=True)\n",
    "winterweather_data.drop(columns={'index'},inplace=True)\n",
    "\n",
    "sum_groupby=winterweather_data[[\"BEGIN_YEAR\",\"DAMAGE_PROPERTY\",\"DAMAGE_CROPS\",\"DEATH_TOLL\",'TOTAL_INJURIES']].groupby([\"BEGIN_YEAR\"]).sum()\n",
    "count_groupby=winterweather_data[[\"BEGIN_YEAR\",\"DAMAGE_PROPERTY\"]].groupby([\"BEGIN_YEAR\"]).count()\n",
    "count_groupby.rename(columns={'DAMAGE_PROPERTY':'WINTERWEATHER_COUNT'}, inplace=True)\n",
    "\n",
    "winterweather_summary=pd.merge(count_groupby,sum_groupby, on=\"BEGIN_YEAR\")\n",
    "winterweather_summary.to_csv(\"Output/WinterWeatherSummary.csv\", index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sumarize Tornado data\n",
    "tornado_data= storm_data.loc[(storm_data['EVENT_TYPE'] == 'Tornado')]\n",
    "\n",
    "tornado_data.reset_index(inplace=True)\n",
    "tornado_data.drop(columns={'index'},inplace=True)\n",
    "\n",
    "sum_groupby=tornado_data[[\"BEGIN_YEAR\",\"DAMAGE_PROPERTY\",\"DAMAGE_CROPS\",\"DEATH_TOLL\",'TOTAL_INJURIES']].groupby([\"BEGIN_YEAR\"]).sum()\n",
    "count_groupby=tornado_data[[\"BEGIN_YEAR\",\"DAMAGE_PROPERTY\"]].groupby([\"BEGIN_YEAR\"]).count()\n",
    "count_groupby.rename(columns={'DAMAGE_PROPERTY':'TORNADO_COUNT'}, inplace=True)\n",
    "\n",
    "tornado_summary=pd.merge(count_groupby,sum_groupby, on=\"BEGIN_YEAR\")\n",
    "tornado_summary.to_csv(\"Output/TornadoSummary.csv\", index=True)\n",
    "\n",
    "tornado_data= tornado_data.loc[(tornado_data['BEGIN_YEAR'] == '2019')]\n",
    "tornado_data.to_csv(\"Output/TornadoData2019.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sumarize Lightning data\n",
    "lightning_data= storm_data.loc[(storm_data['EVENT_TYPE'] == 'Lightning')]\n",
    "\n",
    "lightning_data.reset_index(inplace=True)\n",
    "lightning_data.drop(columns={'index'},inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "sum_groupby=lightning_data[[\"BEGIN_YEAR\",\"DAMAGE_PROPERTY\",\"DAMAGE_CROPS\",\"DEATH_TOLL\",'TOTAL_INJURIES']].groupby([\"BEGIN_YEAR\"]).sum()\n",
    "count_groupby=lightning_data[[\"BEGIN_YEAR\",\"DAMAGE_PROPERTY\"]].groupby([\"BEGIN_YEAR\"]).count()\n",
    "count_groupby.rename(columns={'DAMAGE_PROPERTY':'Lightning_COUNT'}, inplace=True)\n",
    "\n",
    "lightning_summary=pd.merge(count_groupby,sum_groupby, on=\"BEGIN_YEAR\")\n",
    "lightning_summary.to_csv(\"Output/LightningSummary.csv\", index=True)\n",
    "\n",
    "\n",
    "lightning_data= lightning_data.loc[(lightning_data['BEGIN_YEAR'] == '2019')]\n",
    "lightning_data.to_csv(\"Output/LightningData2019.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sumarize Heat data\n",
    "heat_data= storm_data.loc[(storm_data['EVENT_TYPE'] == 'Heat')]\n",
    "\n",
    "heat_data.reset_index(inplace=True)\n",
    "heat_data.drop(columns={'index'},inplace=True)\n",
    "\n",
    "sum_groupby=heat_data[[\"BEGIN_YEAR\",\"DAMAGE_PROPERTY\",\"DAMAGE_CROPS\",\"DEATH_TOLL\",'TOTAL_INJURIES']].groupby([\"BEGIN_YEAR\"]).sum()\n",
    "count_groupby=heat_data[[\"BEGIN_YEAR\",\"DAMAGE_PROPERTY\"]].groupby([\"BEGIN_YEAR\"]).count()\n",
    "count_groupby.rename(columns={'DAMAGE_PROPERTY':'HEAT_COUNT'}, inplace=True)\n",
    "\n",
    "heat_summary=pd.merge(count_groupby,sum_groupby, on=\"BEGIN_YEAR\")\n",
    "heat_summary.to_csv(\"Output/HeatSummary.csv\", index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
